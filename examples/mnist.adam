// MNIST handwritten digit classification — 2-layer neural network
//
// Architecture: 784 → 128 (ReLU) → 10 (Softmax + Cross-Entropy)
// Training: Mini-batch SGD with manual backpropagation
//
// Prerequisites:
//   just prepare-mnist   (downloads and converts MNIST data)
//   just mnist           (compiles and runs this program)

// ── Load data ──────────────────────────────────────────────────────
let train_images = tensor_load("data/mnist_train_images.bin")
let train_labels = tensor_load("data/mnist_train_labels.bin")
let test_images  = tensor_load("data/mnist_test_images.bin")
let test_labels  = tensor_load("data/mnist_test_labels.bin")

println("Data loaded.")
println(tensor_shape(train_images))
println(tensor_shape(train_labels))

// ── Initialize weights (Xavier) ────────────────────────────────────
// Xavier scale for (fan_in → fan_out): sqrt(2 / (fan_in + fan_out))
// 784→128: sqrt(2/912) ≈ 0.047;  128→10: sqrt(2/138) ≈ 0.120
let w1 = tensor_randn([784, 128]) * 0.047
let b1 = tensor_zeros([1, 128])
let w2 = tensor_randn([128, 10]) * 0.120
let b2 = tensor_zeros([1, 10])

// ── Hyperparameters ────────────────────────────────────────────────
let lr = 0.1
let epochs = 5
let batch_size = 32
let num_batches = 1875
let batch_size_f = 32.0
let num_batches_f = 1875.0

// ── Training loop ──────────────────────────────────────────────────
let epoch = 0
while epoch < epochs {
    let batch = 0
    let epoch_loss = 0.0

    while batch < num_batches {
        let start = batch * batch_size

        // ── Mini-batch extraction ──────────────────────────────────
        let x = tensor_slice(train_images, start, batch_size)
        let y = tensor_slice(train_labels, start, batch_size)
        let targets = tensor_one_hot(y, 10)

        // ── Forward pass ───────────────────────────────────────────
        let z1 = x @@ w1 + b1
        let h = tensor_relu(z1)
        let logits = h @@ w2 + b2

        // Numerically stable softmax: shift by global max to prevent overflow
        let max_l = tensor_max(logits)
        let shifted = logits - max_l
        let exps = tensor_exp(shifted)
        let sum_exps = tensor_sum_axis(exps, 1)
        let probs = exps / sum_exps

        // Cross-entropy loss (averaged over batch)
        let log_probs = tensor_log(probs + 1e-8)
        let loss = 0.0 - tensor_sum(targets * log_probs) / batch_size_f
        epoch_loss = epoch_loss + loss

        // ── Backward pass ──────────────────────────────────────────
        // dL/d(logits) for softmax + cross-entropy = (probs - targets) / N
        let d_logits = (probs - targets) / batch_size_f

        // Layer 2 gradients
        let d_w2 = tensor_transpose(h) @@ d_logits
        let d_b2 = tensor_sum_axis(d_logits, 0)

        // Backprop through layer 2 weights
        let d_h = d_logits @@ tensor_transpose(w2)

        // ReLU backward: zero gradient where pre-activation ≤ 0
        let d_z1 = tensor_relu_backward(z1, d_h)

        // Layer 1 gradients
        let d_w1 = tensor_transpose(x) @@ d_z1
        let d_b1 = tensor_sum_axis(d_z1, 0)

        // ── SGD update ─────────────────────────────────────────────
        w1 = w1 - lr * d_w1
        b1 = b1 - lr * d_b1
        w2 = w2 - lr * d_w2
        b2 = b2 - lr * d_b2

        batch = batch + 1
    }

    // ── Epoch summary ──────────────────────────────────────────────
    let avg_loss = epoch_loss / num_batches_f
    print("Epoch ")
    print(epoch + 1)
    print(" — avg loss: ")
    println(avg_loss)

    epoch = epoch + 1
}

// ── Evaluation on test set ─────────────────────────────────────────
// Process test set in batches and count correct predictions
let correct = 0.0
let test_batch = 0
let test_batches = 312

while test_batch < test_batches {
    let start = test_batch * batch_size
    let x = tensor_slice(test_images, start, batch_size)
    let y = tensor_slice(test_labels, start, batch_size)

    // Forward pass
    let z1 = x @@ w1 + b1
    let h = tensor_relu(z1)
    let logits = h @@ w2 + b2

    // Count correct predictions (argmax of logits vs label)
    let i = 0
    while i < batch_size {
        let row_2d = tensor_slice(logits, i, 1)
        let row = tensor_reshape(row_2d, [10])
        let label = tensor_slice(y, i, 1)
        let pred_class = 0.0
        let max_val = 0.0 - 1e9
        let c = 0
        while c < 10 {
            let val = tensor_slice(row, c, 1)
            let v = tensor_sum(val)
            if v > max_val {
                max_val = v
                pred_class = to_float(c)
            } else {
                0.0
            }
            c = c + 1
        }
        let true_label = tensor_sum(label)
        if pred_class == true_label {
            correct = correct + 1.0
        } else {
            0.0
        }
        i = i + 1
    }

    test_batch = test_batch + 1
}

let total = to_float(test_batches * batch_size)
print("Test accuracy: ")
print(correct)
print(" / ")
print(total)
print(" = ")
println(correct / total)
